## AlexNet

### Abstract
* 提出一个深度卷积神经网络，用来在ImageNet竞赛上做图像分类，效果：优于之前最好的结果
* 模型结构上：
    * 拥有6000万参数，650,000个神经元
    * 包含5个卷积层，池化层，3个全连接层，1000路的softmax
* 使用GPU -> 训练快
* 使用dropout -> 减少全连接层的过拟合

### Introduction
* 带标签数据集的规模限制了模型的能力，为了使模型能够识别更多的物体，我们需要使用更大的训练数据集来训练我们的模型
* 为了能够从更多的图片中学习，我们的模型需要有更大的学习能力。而卷积神经网络拥有更少的连接和参数使得容易训练。
* 作者训练了一个深度卷积神经网络，在ImageNet竞赛中，得到了最好的结果。作者针对GPU上的2d卷积操作做了优化。提升了模型精读，减少了训练时间。
* 使用了一些技术防止了过拟合。
* 作者发现，如果从5层卷积层和3层全连接层中拿掉任意一层，最终的结果会比之前的差。

### The Dataset
* 在ImageNet中，使用12,000,000做训练集，50,000做验证集，150,000做测试集。

### The Architecture
* 神经元的输出结果经过一个非线性操作 $f$ ,通常 $f(x) = tanh(x)$ 或者 $f(x) = \frac{1}{1 + e^{-x}}$ ,但随着梯度下降，训练过程很慢。
* 使用ReLU来训练比 $tanh$ 快。如果在我们的大模型中使用其他的非线性操作，我们的训练将无法继续下去。
* 多GPU训练：GPU之间的通信仅在特定的层进行。
* 局部响应归一化
    * 经过卷积层之后，输入 $ReLU$ ,输出结果为 $a_{x, y}^i$ ,表示第 $i$ 个特征响应图中，${x, y}$ 位置的点 ,经过响应归一化之后，得到 $b_{x, y}^i$
    $$b_{x, y}^i = a_{x, y}^i / (k + \alpha \sum\limits_{j = max(0, i - n / 2)}^{min(N - 1. i + n / 2)} (a_{x, y}^j)^2)^\beta$$
    * 解释：与 $a_{x, y}^i$ 毗邻但不在同一特征响应图的点做局部归一化。
    * 超参：$k, n: 临近kernel-map个数, \alpha ,\beta$.
* 重叠池化可能使得模型训练更难过拟合。
* 整体架构
    * 共有8个带参数的层：5个卷积层、3个全连接层
    * 输出结果经过 $1000$ 路的 $softmax$,对应这 $1000$ 类的标签。
    * 第一、二个卷积层之后接入响应归一化层。
    * 最大池化层接在响应归一化层和第五卷积层之后
    * 每个卷积层和全连接层之后都使用 $ReLU$

![](../imgs/alexnet.jpg)

### 减少过拟合

### 学习细节

### 结果

### 讨论

