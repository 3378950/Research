## ResNet

### 解决的问题
网络的深度确实是影响网络精度的重要指标，但是更深的网络却存在梯度消失/爆炸问题，影响收敛，这个问题已经在很大程度上通过归一化初始化和中间归一化层来解决。
* 更深的网络能够开始收敛时，暴露了一个退化问题：随着网络深度的增加，准确率达到饱和，然后迅速下降。这种退化不是由过拟合引起的，并且在适当的深度模型上添加更多的层会导致更高的训练误差。

### 主要贡献
* 构建到更深层模型的解决方案：通过恒等映射（identity mapping）来构建增加的层，而其它层直接从浅层模型中复制而来。该解的存在性表明，更深层次的模型不应比较浅的模型产生更高的训练误差。
* 提出残差学习框架解决退化问题，使得网络很容易优化，并可以显著增加深度来提高准确性。

### 深度残差学习

#### 残差学习
 $H(x)$ 看作是一些堆叠层(不一定是整个网络)组成的底层映射，用 $x$ 表示这些层中的第一个层的输入。如果假设多个非线性层可以近似于复杂函数，那么它等价于假设他们可以近似于残差函数，即 $H(X)−x$ (假设输入和输出具有相同的维度)。因此，与其期望叠加层近似 $H(X)$ ，我们不如显式地让这些层近似一个残差函数 $F(x)：=H(x)−x$ 。原来的函数因此变成 $F(x)+x$ 。
如果可以将添加的层构造为恒等映射，则更深层次的模型应该具有不大于其浅层结构的训练误差。

#### 恒等映射
$$ y = F(x, {w_i}) + x$$
其中 $f$ 与 $x$ 的维度必须相等，对于不一致的情况，可通过快捷连接执行线性投影 $w_s$ 以匹配维度。

